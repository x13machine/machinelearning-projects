{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os, math\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HPI_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa = df[(df['level'] == 'USA or Census Division') & (df['place_name'] == 'United States') & (df['hpi_flavor'] == 'all-transactions')][['yr','period','index_nsa']]\n",
    "usa['time'] =usa['yr'].astype(str) + '-' + usa['period'].astype(str)\n",
    "usa = usa[['time','index_nsa']]\n",
    "usa['change'] = usa['index_nsa'].shift(-1) / usa['index_nsa']\n",
    "indice = dict(zip(usa['time'],usa['change']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['hpi_flavor'] == 'all-transactions') & (df['level'] == 'MSA')]\n",
    "df['time'] =df['yr'].astype(str) + '-' + df['period'].astype(str)\n",
    "df = pd.pivot_table(df, values = 'index_nsa', index=['place_name'], columns = 'time').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>place_name</th>\n",
       "      <th>1975-1</th>\n",
       "      <th>1975-2</th>\n",
       "      <th>1975-3</th>\n",
       "      <th>1975-4</th>\n",
       "      <th>1976-1</th>\n",
       "      <th>1976-2</th>\n",
       "      <th>1976-3</th>\n",
       "      <th>1976-4</th>\n",
       "      <th>1977-1</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-1</th>\n",
       "      <th>2017-2</th>\n",
       "      <th>2017-3</th>\n",
       "      <th>2017-4</th>\n",
       "      <th>2018-1</th>\n",
       "      <th>2018-2</th>\n",
       "      <th>2018-3</th>\n",
       "      <th>2018-4</th>\n",
       "      <th>2019-1</th>\n",
       "      <th>2019-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>199.05</td>\n",
       "      <td>210.11</td>\n",
       "      <td>213.20</td>\n",
       "      <td>210.09</td>\n",
       "      <td>211.23</td>\n",
       "      <td>225.27</td>\n",
       "      <td>222.77</td>\n",
       "      <td>228.00</td>\n",
       "      <td>229.03</td>\n",
       "      <td>228.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akron, OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.01</td>\n",
       "      <td>41.17</td>\n",
       "      <td>...</td>\n",
       "      <td>144.58</td>\n",
       "      <td>149.08</td>\n",
       "      <td>150.69</td>\n",
       "      <td>152.28</td>\n",
       "      <td>152.30</td>\n",
       "      <td>154.68</td>\n",
       "      <td>161.16</td>\n",
       "      <td>158.05</td>\n",
       "      <td>160.22</td>\n",
       "      <td>165.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albany, GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>146.45</td>\n",
       "      <td>148.15</td>\n",
       "      <td>151.02</td>\n",
       "      <td>152.56</td>\n",
       "      <td>157.75</td>\n",
       "      <td>153.04</td>\n",
       "      <td>148.83</td>\n",
       "      <td>155.63</td>\n",
       "      <td>153.55</td>\n",
       "      <td>163.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albany-Lebanon, OR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>212.82</td>\n",
       "      <td>218.65</td>\n",
       "      <td>225.32</td>\n",
       "      <td>231.10</td>\n",
       "      <td>236.76</td>\n",
       "      <td>244.53</td>\n",
       "      <td>252.69</td>\n",
       "      <td>259.02</td>\n",
       "      <td>262.90</td>\n",
       "      <td>268.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany-Schenectady-Troy, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>193.94</td>\n",
       "      <td>196.05</td>\n",
       "      <td>199.49</td>\n",
       "      <td>200.20</td>\n",
       "      <td>202.51</td>\n",
       "      <td>202.70</td>\n",
       "      <td>206.08</td>\n",
       "      <td>204.93</td>\n",
       "      <td>206.50</td>\n",
       "      <td>209.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>164.30</td>\n",
       "      <td>166.79</td>\n",
       "      <td>171.19</td>\n",
       "      <td>171.02</td>\n",
       "      <td>172.14</td>\n",
       "      <td>176.41</td>\n",
       "      <td>176.90</td>\n",
       "      <td>176.27</td>\n",
       "      <td>179.10</td>\n",
       "      <td>184.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexandria, LA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>201.37</td>\n",
       "      <td>207.35</td>\n",
       "      <td>213.73</td>\n",
       "      <td>216.93</td>\n",
       "      <td>212.73</td>\n",
       "      <td>215.17</td>\n",
       "      <td>211.19</td>\n",
       "      <td>210.62</td>\n",
       "      <td>215.33</td>\n",
       "      <td>213.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Allentown-Bethlehem-Easton, PA-NJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>171.88</td>\n",
       "      <td>174.52</td>\n",
       "      <td>177.20</td>\n",
       "      <td>178.75</td>\n",
       "      <td>179.70</td>\n",
       "      <td>185.52</td>\n",
       "      <td>187.04</td>\n",
       "      <td>186.76</td>\n",
       "      <td>188.11</td>\n",
       "      <td>192.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Altoona, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>193.55</td>\n",
       "      <td>189.55</td>\n",
       "      <td>198.70</td>\n",
       "      <td>198.80</td>\n",
       "      <td>198.11</td>\n",
       "      <td>206.58</td>\n",
       "      <td>201.50</td>\n",
       "      <td>200.87</td>\n",
       "      <td>194.35</td>\n",
       "      <td>209.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amarillo, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>190.44</td>\n",
       "      <td>192.50</td>\n",
       "      <td>193.99</td>\n",
       "      <td>195.53</td>\n",
       "      <td>195.77</td>\n",
       "      <td>198.75</td>\n",
       "      <td>201.19</td>\n",
       "      <td>200.79</td>\n",
       "      <td>200.77</td>\n",
       "      <td>205.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ames, IA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>192.70</td>\n",
       "      <td>200.96</td>\n",
       "      <td>205.14</td>\n",
       "      <td>206.29</td>\n",
       "      <td>201.11</td>\n",
       "      <td>211.47</td>\n",
       "      <td>210.09</td>\n",
       "      <td>209.72</td>\n",
       "      <td>212.75</td>\n",
       "      <td>217.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Anaheim-Santa Ana-Irvine, CA (MSAD)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.62</td>\n",
       "      <td>24.09</td>\n",
       "      <td>24.85</td>\n",
       "      <td>26.47</td>\n",
       "      <td>27.68</td>\n",
       "      <td>29.15</td>\n",
       "      <td>30.63</td>\n",
       "      <td>32.55</td>\n",
       "      <td>...</td>\n",
       "      <td>326.47</td>\n",
       "      <td>332.48</td>\n",
       "      <td>337.30</td>\n",
       "      <td>341.65</td>\n",
       "      <td>348.33</td>\n",
       "      <td>354.96</td>\n",
       "      <td>358.54</td>\n",
       "      <td>360.50</td>\n",
       "      <td>362.38</td>\n",
       "      <td>364.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Anchorage, AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>226.31</td>\n",
       "      <td>227.37</td>\n",
       "      <td>227.94</td>\n",
       "      <td>231.18</td>\n",
       "      <td>225.64</td>\n",
       "      <td>229.56</td>\n",
       "      <td>229.27</td>\n",
       "      <td>228.83</td>\n",
       "      <td>236.33</td>\n",
       "      <td>234.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ann Arbor, MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>190.20</td>\n",
       "      <td>196.77</td>\n",
       "      <td>201.26</td>\n",
       "      <td>202.39</td>\n",
       "      <td>203.30</td>\n",
       "      <td>212.78</td>\n",
       "      <td>218.99</td>\n",
       "      <td>219.20</td>\n",
       "      <td>222.49</td>\n",
       "      <td>226.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anniston-Oxford, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>153.81</td>\n",
       "      <td>159.74</td>\n",
       "      <td>163.47</td>\n",
       "      <td>164.78</td>\n",
       "      <td>168.36</td>\n",
       "      <td>166.42</td>\n",
       "      <td>175.50</td>\n",
       "      <td>169.79</td>\n",
       "      <td>178.65</td>\n",
       "      <td>175.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Appleton, WI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>164.20</td>\n",
       "      <td>169.74</td>\n",
       "      <td>173.11</td>\n",
       "      <td>175.42</td>\n",
       "      <td>174.53</td>\n",
       "      <td>182.17</td>\n",
       "      <td>186.39</td>\n",
       "      <td>187.97</td>\n",
       "      <td>191.78</td>\n",
       "      <td>193.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Asheville, NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>260.45</td>\n",
       "      <td>268.56</td>\n",
       "      <td>270.57</td>\n",
       "      <td>273.14</td>\n",
       "      <td>281.92</td>\n",
       "      <td>288.94</td>\n",
       "      <td>295.93</td>\n",
       "      <td>295.45</td>\n",
       "      <td>299.36</td>\n",
       "      <td>307.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Athens-Clarke County, GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>192.10</td>\n",
       "      <td>195.28</td>\n",
       "      <td>200.23</td>\n",
       "      <td>205.84</td>\n",
       "      <td>206.98</td>\n",
       "      <td>216.40</td>\n",
       "      <td>222.95</td>\n",
       "      <td>220.43</td>\n",
       "      <td>229.60</td>\n",
       "      <td>231.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Atlanta-Sandy Springs-Alpharetta, GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.13</td>\n",
       "      <td>38.25</td>\n",
       "      <td>40.90</td>\n",
       "      <td>42.49</td>\n",
       "      <td>41.43</td>\n",
       "      <td>41.80</td>\n",
       "      <td>...</td>\n",
       "      <td>188.09</td>\n",
       "      <td>194.98</td>\n",
       "      <td>198.42</td>\n",
       "      <td>201.12</td>\n",
       "      <td>207.39</td>\n",
       "      <td>212.63</td>\n",
       "      <td>219.06</td>\n",
       "      <td>219.74</td>\n",
       "      <td>222.42</td>\n",
       "      <td>229.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Atlantic City-Hammonton, NJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>187.30</td>\n",
       "      <td>189.92</td>\n",
       "      <td>192.69</td>\n",
       "      <td>192.40</td>\n",
       "      <td>195.51</td>\n",
       "      <td>201.62</td>\n",
       "      <td>200.75</td>\n",
       "      <td>200.82</td>\n",
       "      <td>204.29</td>\n",
       "      <td>207.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Auburn-Opelika, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>185.62</td>\n",
       "      <td>187.48</td>\n",
       "      <td>189.09</td>\n",
       "      <td>187.96</td>\n",
       "      <td>194.14</td>\n",
       "      <td>197.54</td>\n",
       "      <td>199.45</td>\n",
       "      <td>208.08</td>\n",
       "      <td>213.49</td>\n",
       "      <td>214.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Augusta-Richmond County, GA-SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>167.77</td>\n",
       "      <td>167.91</td>\n",
       "      <td>172.87</td>\n",
       "      <td>171.30</td>\n",
       "      <td>178.46</td>\n",
       "      <td>181.97</td>\n",
       "      <td>181.57</td>\n",
       "      <td>183.93</td>\n",
       "      <td>179.64</td>\n",
       "      <td>186.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Austin-Round Rock-Georgetown, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>296.12</td>\n",
       "      <td>306.19</td>\n",
       "      <td>309.86</td>\n",
       "      <td>312.22</td>\n",
       "      <td>316.78</td>\n",
       "      <td>324.83</td>\n",
       "      <td>332.01</td>\n",
       "      <td>331.96</td>\n",
       "      <td>339.75</td>\n",
       "      <td>345.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>197.66</td>\n",
       "      <td>201.98</td>\n",
       "      <td>203.10</td>\n",
       "      <td>206.67</td>\n",
       "      <td>208.27</td>\n",
       "      <td>213.09</td>\n",
       "      <td>216.09</td>\n",
       "      <td>217.78</td>\n",
       "      <td>220.23</td>\n",
       "      <td>221.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Baltimore-Columbia-Towson, MD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.33</td>\n",
       "      <td>35.05</td>\n",
       "      <td>35.34</td>\n",
       "      <td>35.94</td>\n",
       "      <td>...</td>\n",
       "      <td>219.48</td>\n",
       "      <td>222.90</td>\n",
       "      <td>225.40</td>\n",
       "      <td>228.31</td>\n",
       "      <td>230.44</td>\n",
       "      <td>232.02</td>\n",
       "      <td>232.30</td>\n",
       "      <td>231.61</td>\n",
       "      <td>234.64</td>\n",
       "      <td>238.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bangor, ME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>184.14</td>\n",
       "      <td>180.21</td>\n",
       "      <td>178.96</td>\n",
       "      <td>188.48</td>\n",
       "      <td>195.19</td>\n",
       "      <td>190.16</td>\n",
       "      <td>191.36</td>\n",
       "      <td>196.28</td>\n",
       "      <td>206.68</td>\n",
       "      <td>195.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Barnstable Town, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>282.28</td>\n",
       "      <td>290.38</td>\n",
       "      <td>295.61</td>\n",
       "      <td>299.90</td>\n",
       "      <td>303.60</td>\n",
       "      <td>311.80</td>\n",
       "      <td>316.09</td>\n",
       "      <td>309.72</td>\n",
       "      <td>314.74</td>\n",
       "      <td>323.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>209.48</td>\n",
       "      <td>215.68</td>\n",
       "      <td>219.68</td>\n",
       "      <td>220.65</td>\n",
       "      <td>221.35</td>\n",
       "      <td>224.00</td>\n",
       "      <td>225.53</td>\n",
       "      <td>225.02</td>\n",
       "      <td>228.13</td>\n",
       "      <td>230.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Battle Creek, MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>158.56</td>\n",
       "      <td>163.21</td>\n",
       "      <td>169.67</td>\n",
       "      <td>163.91</td>\n",
       "      <td>164.88</td>\n",
       "      <td>170.94</td>\n",
       "      <td>175.89</td>\n",
       "      <td>177.54</td>\n",
       "      <td>176.19</td>\n",
       "      <td>183.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bay City, MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>146.53</td>\n",
       "      <td>154.01</td>\n",
       "      <td>151.38</td>\n",
       "      <td>149.58</td>\n",
       "      <td>155.49</td>\n",
       "      <td>154.99</td>\n",
       "      <td>161.88</td>\n",
       "      <td>162.88</td>\n",
       "      <td>163.75</td>\n",
       "      <td>165.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Vallejo, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>249.12</td>\n",
       "      <td>257.81</td>\n",
       "      <td>260.78</td>\n",
       "      <td>266.58</td>\n",
       "      <td>271.23</td>\n",
       "      <td>282.93</td>\n",
       "      <td>286.98</td>\n",
       "      <td>289.09</td>\n",
       "      <td>289.08</td>\n",
       "      <td>292.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Victoria, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>218.61</td>\n",
       "      <td>218.43</td>\n",
       "      <td>217.97</td>\n",
       "      <td>212.68</td>\n",
       "      <td>222.43</td>\n",
       "      <td>224.83</td>\n",
       "      <td>222.60</td>\n",
       "      <td>218.64</td>\n",
       "      <td>230.12</td>\n",
       "      <td>226.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Vineland-Bridgeton, NJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>168.18</td>\n",
       "      <td>170.61</td>\n",
       "      <td>164.75</td>\n",
       "      <td>174.03</td>\n",
       "      <td>170.10</td>\n",
       "      <td>168.10</td>\n",
       "      <td>170.20</td>\n",
       "      <td>174.18</td>\n",
       "      <td>176.70</td>\n",
       "      <td>186.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Virginia Beach-Norfolk-Newport News, VA-NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.67</td>\n",
       "      <td>38.73</td>\n",
       "      <td>40.21</td>\n",
       "      <td>...</td>\n",
       "      <td>219.68</td>\n",
       "      <td>223.83</td>\n",
       "      <td>226.55</td>\n",
       "      <td>227.42</td>\n",
       "      <td>227.73</td>\n",
       "      <td>230.37</td>\n",
       "      <td>232.75</td>\n",
       "      <td>229.62</td>\n",
       "      <td>236.06</td>\n",
       "      <td>237.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Visalia, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>188.61</td>\n",
       "      <td>191.84</td>\n",
       "      <td>195.87</td>\n",
       "      <td>200.58</td>\n",
       "      <td>201.28</td>\n",
       "      <td>208.14</td>\n",
       "      <td>209.39</td>\n",
       "      <td>208.96</td>\n",
       "      <td>212.37</td>\n",
       "      <td>216.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Waco, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>203.07</td>\n",
       "      <td>212.18</td>\n",
       "      <td>216.39</td>\n",
       "      <td>218.66</td>\n",
       "      <td>227.03</td>\n",
       "      <td>233.59</td>\n",
       "      <td>236.57</td>\n",
       "      <td>236.28</td>\n",
       "      <td>236.91</td>\n",
       "      <td>244.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Walla Walla, WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>219.55</td>\n",
       "      <td>229.61</td>\n",
       "      <td>230.66</td>\n",
       "      <td>234.90</td>\n",
       "      <td>233.06</td>\n",
       "      <td>235.22</td>\n",
       "      <td>244.91</td>\n",
       "      <td>254.88</td>\n",
       "      <td>258.83</td>\n",
       "      <td>261.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Warner Robins, GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>145.97</td>\n",
       "      <td>143.97</td>\n",
       "      <td>150.61</td>\n",
       "      <td>146.80</td>\n",
       "      <td>152.81</td>\n",
       "      <td>154.64</td>\n",
       "      <td>157.75</td>\n",
       "      <td>155.19</td>\n",
       "      <td>152.04</td>\n",
       "      <td>163.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Warren-Troy-Farmington Hills, MI (MSAD)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.65</td>\n",
       "      <td>32.93</td>\n",
       "      <td>34.08</td>\n",
       "      <td>34.63</td>\n",
       "      <td>34.79</td>\n",
       "      <td>35.41</td>\n",
       "      <td>35.89</td>\n",
       "      <td>...</td>\n",
       "      <td>167.12</td>\n",
       "      <td>171.78</td>\n",
       "      <td>175.06</td>\n",
       "      <td>176.84</td>\n",
       "      <td>179.27</td>\n",
       "      <td>184.99</td>\n",
       "      <td>188.72</td>\n",
       "      <td>188.88</td>\n",
       "      <td>190.39</td>\n",
       "      <td>194.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Washington-Arlington-Alexandria, DC-VA-MD-WV (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.41</td>\n",
       "      <td>30.93</td>\n",
       "      <td>30.92</td>\n",
       "      <td>32.29</td>\n",
       "      <td>33.37</td>\n",
       "      <td>33.22</td>\n",
       "      <td>33.33</td>\n",
       "      <td>...</td>\n",
       "      <td>258.15</td>\n",
       "      <td>266.42</td>\n",
       "      <td>267.44</td>\n",
       "      <td>269.77</td>\n",
       "      <td>273.02</td>\n",
       "      <td>277.13</td>\n",
       "      <td>280.63</td>\n",
       "      <td>282.95</td>\n",
       "      <td>284.43</td>\n",
       "      <td>290.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>212.82</td>\n",
       "      <td>219.97</td>\n",
       "      <td>223.61</td>\n",
       "      <td>222.63</td>\n",
       "      <td>228.05</td>\n",
       "      <td>226.04</td>\n",
       "      <td>232.83</td>\n",
       "      <td>234.72</td>\n",
       "      <td>230.79</td>\n",
       "      <td>234.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Watertown-Fort Drum, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>208.61</td>\n",
       "      <td>202.65</td>\n",
       "      <td>189.39</td>\n",
       "      <td>212.59</td>\n",
       "      <td>222.38</td>\n",
       "      <td>198.94</td>\n",
       "      <td>214.71</td>\n",
       "      <td>220.51</td>\n",
       "      <td>226.82</td>\n",
       "      <td>226.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Wausau-Weston, WI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>169.35</td>\n",
       "      <td>172.59</td>\n",
       "      <td>174.58</td>\n",
       "      <td>176.59</td>\n",
       "      <td>178.78</td>\n",
       "      <td>185.22</td>\n",
       "      <td>185.17</td>\n",
       "      <td>186.86</td>\n",
       "      <td>188.81</td>\n",
       "      <td>193.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Weirton-Steubenville, WV-OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>171.23</td>\n",
       "      <td>186.27</td>\n",
       "      <td>178.92</td>\n",
       "      <td>178.65</td>\n",
       "      <td>177.39</td>\n",
       "      <td>181.11</td>\n",
       "      <td>190.05</td>\n",
       "      <td>186.00</td>\n",
       "      <td>193.52</td>\n",
       "      <td>193.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Wenatchee, WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>216.98</td>\n",
       "      <td>222.73</td>\n",
       "      <td>229.23</td>\n",
       "      <td>238.45</td>\n",
       "      <td>239.43</td>\n",
       "      <td>249.94</td>\n",
       "      <td>259.57</td>\n",
       "      <td>261.10</td>\n",
       "      <td>269.22</td>\n",
       "      <td>268.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>West Palm Beach-Boca Raton-Boynton Beach, FL (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>269.34</td>\n",
       "      <td>278.39</td>\n",
       "      <td>283.16</td>\n",
       "      <td>287.13</td>\n",
       "      <td>292.97</td>\n",
       "      <td>298.18</td>\n",
       "      <td>306.35</td>\n",
       "      <td>310.50</td>\n",
       "      <td>310.69</td>\n",
       "      <td>311.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Wheeling, WV-OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>181.87</td>\n",
       "      <td>187.25</td>\n",
       "      <td>190.46</td>\n",
       "      <td>186.56</td>\n",
       "      <td>194.97</td>\n",
       "      <td>195.54</td>\n",
       "      <td>196.99</td>\n",
       "      <td>200.86</td>\n",
       "      <td>197.27</td>\n",
       "      <td>198.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Wichita Falls, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>173.95</td>\n",
       "      <td>171.29</td>\n",
       "      <td>172.55</td>\n",
       "      <td>180.74</td>\n",
       "      <td>176.82</td>\n",
       "      <td>184.79</td>\n",
       "      <td>180.38</td>\n",
       "      <td>184.56</td>\n",
       "      <td>179.49</td>\n",
       "      <td>190.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Wichita, KS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.32</td>\n",
       "      <td>53.73</td>\n",
       "      <td>...</td>\n",
       "      <td>168.26</td>\n",
       "      <td>174.56</td>\n",
       "      <td>177.90</td>\n",
       "      <td>177.70</td>\n",
       "      <td>177.82</td>\n",
       "      <td>180.90</td>\n",
       "      <td>184.41</td>\n",
       "      <td>183.31</td>\n",
       "      <td>186.34</td>\n",
       "      <td>191.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Williamsport, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>191.63</td>\n",
       "      <td>201.26</td>\n",
       "      <td>198.51</td>\n",
       "      <td>199.20</td>\n",
       "      <td>200.42</td>\n",
       "      <td>198.21</td>\n",
       "      <td>205.51</td>\n",
       "      <td>200.56</td>\n",
       "      <td>196.85</td>\n",
       "      <td>201.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Wilmington, DE-MD-NJ (MSAD)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>187.13</td>\n",
       "      <td>188.05</td>\n",
       "      <td>189.35</td>\n",
       "      <td>191.10</td>\n",
       "      <td>194.94</td>\n",
       "      <td>194.68</td>\n",
       "      <td>197.75</td>\n",
       "      <td>196.67</td>\n",
       "      <td>200.48</td>\n",
       "      <td>204.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Wilmington, NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>203.44</td>\n",
       "      <td>211.12</td>\n",
       "      <td>217.98</td>\n",
       "      <td>219.39</td>\n",
       "      <td>220.30</td>\n",
       "      <td>228.47</td>\n",
       "      <td>232.18</td>\n",
       "      <td>234.34</td>\n",
       "      <td>237.78</td>\n",
       "      <td>244.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Winchester, VA-WV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>200.34</td>\n",
       "      <td>206.94</td>\n",
       "      <td>210.96</td>\n",
       "      <td>210.32</td>\n",
       "      <td>216.79</td>\n",
       "      <td>210.40</td>\n",
       "      <td>211.64</td>\n",
       "      <td>222.29</td>\n",
       "      <td>223.40</td>\n",
       "      <td>224.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Winston-Salem, NC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>152.77</td>\n",
       "      <td>159.31</td>\n",
       "      <td>160.60</td>\n",
       "      <td>161.61</td>\n",
       "      <td>167.04</td>\n",
       "      <td>167.62</td>\n",
       "      <td>172.46</td>\n",
       "      <td>175.91</td>\n",
       "      <td>175.37</td>\n",
       "      <td>179.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Worcester, MA-CT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>213.80</td>\n",
       "      <td>219.92</td>\n",
       "      <td>223.56</td>\n",
       "      <td>225.38</td>\n",
       "      <td>228.88</td>\n",
       "      <td>234.84</td>\n",
       "      <td>238.97</td>\n",
       "      <td>239.11</td>\n",
       "      <td>241.20</td>\n",
       "      <td>244.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Yakima, WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>181.63</td>\n",
       "      <td>183.86</td>\n",
       "      <td>190.90</td>\n",
       "      <td>193.78</td>\n",
       "      <td>199.89</td>\n",
       "      <td>205.44</td>\n",
       "      <td>213.52</td>\n",
       "      <td>215.54</td>\n",
       "      <td>217.71</td>\n",
       "      <td>224.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>York-Hanover, PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>167.36</td>\n",
       "      <td>168.65</td>\n",
       "      <td>171.21</td>\n",
       "      <td>172.17</td>\n",
       "      <td>173.90</td>\n",
       "      <td>172.55</td>\n",
       "      <td>177.03</td>\n",
       "      <td>178.71</td>\n",
       "      <td>178.03</td>\n",
       "      <td>181.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Youngstown-Warren-Boardman, OH-PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>150.05</td>\n",
       "      <td>153.73</td>\n",
       "      <td>154.03</td>\n",
       "      <td>157.80</td>\n",
       "      <td>154.69</td>\n",
       "      <td>159.50</td>\n",
       "      <td>159.90</td>\n",
       "      <td>163.65</td>\n",
       "      <td>162.30</td>\n",
       "      <td>165.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Yuba City, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>215.58</td>\n",
       "      <td>221.57</td>\n",
       "      <td>230.93</td>\n",
       "      <td>230.90</td>\n",
       "      <td>234.95</td>\n",
       "      <td>241.88</td>\n",
       "      <td>247.62</td>\n",
       "      <td>248.14</td>\n",
       "      <td>252.09</td>\n",
       "      <td>257.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Yuma, AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>171.58</td>\n",
       "      <td>174.75</td>\n",
       "      <td>177.59</td>\n",
       "      <td>180.66</td>\n",
       "      <td>180.92</td>\n",
       "      <td>180.64</td>\n",
       "      <td>186.50</td>\n",
       "      <td>190.10</td>\n",
       "      <td>190.08</td>\n",
       "      <td>194.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time                                         place_name  1975-1  1975-2  \\\n",
       "0                                           Abilene, TX     NaN     NaN   \n",
       "1                                             Akron, OH     NaN     NaN   \n",
       "2                                            Albany, GA     NaN     NaN   \n",
       "3                                    Albany-Lebanon, OR     NaN     NaN   \n",
       "4                           Albany-Schenectady-Troy, NY     NaN     NaN   \n",
       "5                                       Albuquerque, NM     NaN     NaN   \n",
       "6                                        Alexandria, LA     NaN     NaN   \n",
       "7                     Allentown-Bethlehem-Easton, PA-NJ     NaN     NaN   \n",
       "8                                           Altoona, PA     NaN     NaN   \n",
       "9                                          Amarillo, TX     NaN     NaN   \n",
       "10                                             Ames, IA     NaN     NaN   \n",
       "11                  Anaheim-Santa Ana-Irvine, CA (MSAD)     NaN   23.62   \n",
       "12                                        Anchorage, AK     NaN     NaN   \n",
       "13                                        Ann Arbor, MI     NaN     NaN   \n",
       "14                                  Anniston-Oxford, AL     NaN     NaN   \n",
       "15                                         Appleton, WI     NaN     NaN   \n",
       "16                                        Asheville, NC     NaN     NaN   \n",
       "17                             Athens-Clarke County, GA     NaN     NaN   \n",
       "18                 Atlanta-Sandy Springs-Alpharetta, GA     NaN     NaN   \n",
       "19                          Atlantic City-Hammonton, NJ     NaN     NaN   \n",
       "20                                   Auburn-Opelika, AL     NaN     NaN   \n",
       "21                       Augusta-Richmond County, GA-SC     NaN     NaN   \n",
       "22                     Austin-Round Rock-Georgetown, TX     NaN     NaN   \n",
       "23                                      Bakersfield, CA     NaN     NaN   \n",
       "24                        Baltimore-Columbia-Towson, MD     NaN     NaN   \n",
       "25                                           Bangor, ME     NaN     NaN   \n",
       "26                                  Barnstable Town, MA     NaN     NaN   \n",
       "27                                      Baton Rouge, LA     NaN     NaN   \n",
       "28                                     Battle Creek, MI     NaN     NaN   \n",
       "29                                         Bay City, MI     NaN     NaN   \n",
       "..                                                  ...     ...     ...   \n",
       "374                                         Vallejo, CA     NaN     NaN   \n",
       "375                                        Victoria, TX     NaN     NaN   \n",
       "376                              Vineland-Bridgeton, NJ     NaN     NaN   \n",
       "377          Virginia Beach-Norfolk-Newport News, VA-NC     NaN     NaN   \n",
       "378                                         Visalia, CA     NaN     NaN   \n",
       "379                                            Waco, TX     NaN     NaN   \n",
       "380                                     Walla Walla, WA     NaN     NaN   \n",
       "381                                   Warner Robins, GA     NaN     NaN   \n",
       "382             Warren-Troy-Farmington Hills, MI (MSAD)     NaN     NaN   \n",
       "383   Washington-Arlington-Alexandria, DC-VA-MD-WV (...     NaN     NaN   \n",
       "384                            Waterloo-Cedar Falls, IA     NaN     NaN   \n",
       "385                             Watertown-Fort Drum, NY     NaN     NaN   \n",
       "386                                   Wausau-Weston, WI     NaN     NaN   \n",
       "387                         Weirton-Steubenville, WV-OH     NaN     NaN   \n",
       "388                                       Wenatchee, WA     NaN     NaN   \n",
       "389   West Palm Beach-Boca Raton-Boynton Beach, FL (...     NaN     NaN   \n",
       "390                                     Wheeling, WV-OH     NaN     NaN   \n",
       "391                                   Wichita Falls, TX     NaN     NaN   \n",
       "392                                         Wichita, KS     NaN     NaN   \n",
       "393                                    Williamsport, PA     NaN     NaN   \n",
       "394                         Wilmington, DE-MD-NJ (MSAD)     NaN     NaN   \n",
       "395                                      Wilmington, NC     NaN     NaN   \n",
       "396                                   Winchester, VA-WV     NaN     NaN   \n",
       "397                                   Winston-Salem, NC     NaN     NaN   \n",
       "398                                    Worcester, MA-CT     NaN     NaN   \n",
       "399                                          Yakima, WA     NaN     NaN   \n",
       "400                                    York-Hanover, PA     NaN     NaN   \n",
       "401                   Youngstown-Warren-Boardman, OH-PA     NaN     NaN   \n",
       "402                                       Yuba City, CA     NaN     NaN   \n",
       "403                                            Yuma, AZ     NaN     NaN   \n",
       "\n",
       "time  1975-3  1975-4  1976-1  1976-2  1976-3  1976-4  1977-1   ...    2017-1  \\\n",
       "0        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    199.05   \n",
       "1        NaN     NaN     NaN     NaN     NaN   39.01   41.17   ...    144.58   \n",
       "2        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    146.45   \n",
       "3        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    212.82   \n",
       "4        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    193.94   \n",
       "5        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    164.30   \n",
       "6        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    201.37   \n",
       "7        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    171.88   \n",
       "8        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    193.55   \n",
       "9        NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    190.44   \n",
       "10       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    192.70   \n",
       "11     24.09   24.85   26.47   27.68   29.15   30.63   32.55   ...    326.47   \n",
       "12       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    226.31   \n",
       "13       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    190.20   \n",
       "14       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    153.81   \n",
       "15       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    164.20   \n",
       "16       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    260.45   \n",
       "17       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    192.10   \n",
       "18       NaN   41.13   38.25   40.90   42.49   41.43   41.80   ...    188.09   \n",
       "19       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    187.30   \n",
       "20       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    185.62   \n",
       "21       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    167.77   \n",
       "22       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    296.12   \n",
       "23       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    197.66   \n",
       "24       NaN     NaN     NaN   34.33   35.05   35.34   35.94   ...    219.48   \n",
       "25       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    184.14   \n",
       "26       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    282.28   \n",
       "27       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    209.48   \n",
       "28       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    158.56   \n",
       "29       NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    146.53   \n",
       "..       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "374      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    249.12   \n",
       "375      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    218.61   \n",
       "376      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    168.18   \n",
       "377      NaN     NaN     NaN     NaN   38.67   38.73   40.21   ...    219.68   \n",
       "378      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    188.61   \n",
       "379      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    203.07   \n",
       "380      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    219.55   \n",
       "381      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    145.97   \n",
       "382    33.65   32.93   34.08   34.63   34.79   35.41   35.89   ...    167.12   \n",
       "383    31.41   30.93   30.92   32.29   33.37   33.22   33.33   ...    258.15   \n",
       "384      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    212.82   \n",
       "385      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    208.61   \n",
       "386      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    169.35   \n",
       "387      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    171.23   \n",
       "388      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    216.98   \n",
       "389      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    269.34   \n",
       "390      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    181.87   \n",
       "391      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    173.95   \n",
       "392      NaN     NaN     NaN     NaN     NaN   54.32   53.73   ...    168.26   \n",
       "393      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    191.63   \n",
       "394      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    187.13   \n",
       "395      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    203.44   \n",
       "396      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    200.34   \n",
       "397      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    152.77   \n",
       "398      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    213.80   \n",
       "399      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    181.63   \n",
       "400      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    167.36   \n",
       "401      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    150.05   \n",
       "402      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    215.58   \n",
       "403      NaN     NaN     NaN     NaN     NaN     NaN     NaN   ...    171.58   \n",
       "\n",
       "time  2017-2  2017-3  2017-4  2018-1  2018-2  2018-3  2018-4  2019-1  2019-2  \n",
       "0     210.11  213.20  210.09  211.23  225.27  222.77  228.00  229.03  228.43  \n",
       "1     149.08  150.69  152.28  152.30  154.68  161.16  158.05  160.22  165.23  \n",
       "2     148.15  151.02  152.56  157.75  153.04  148.83  155.63  153.55  163.30  \n",
       "3     218.65  225.32  231.10  236.76  244.53  252.69  259.02  262.90  268.25  \n",
       "4     196.05  199.49  200.20  202.51  202.70  206.08  204.93  206.50  209.04  \n",
       "5     166.79  171.19  171.02  172.14  176.41  176.90  176.27  179.10  184.36  \n",
       "6     207.35  213.73  216.93  212.73  215.17  211.19  210.62  215.33  213.29  \n",
       "7     174.52  177.20  178.75  179.70  185.52  187.04  186.76  188.11  192.35  \n",
       "8     189.55  198.70  198.80  198.11  206.58  201.50  200.87  194.35  209.48  \n",
       "9     192.50  193.99  195.53  195.77  198.75  201.19  200.79  200.77  205.32  \n",
       "10    200.96  205.14  206.29  201.11  211.47  210.09  209.72  212.75  217.99  \n",
       "11    332.48  337.30  341.65  348.33  354.96  358.54  360.50  362.38  364.34  \n",
       "12    227.37  227.94  231.18  225.64  229.56  229.27  228.83  236.33  234.76  \n",
       "13    196.77  201.26  202.39  203.30  212.78  218.99  219.20  222.49  226.01  \n",
       "14    159.74  163.47  164.78  168.36  166.42  175.50  169.79  178.65  175.44  \n",
       "15    169.74  173.11  175.42  174.53  182.17  186.39  187.97  191.78  193.34  \n",
       "16    268.56  270.57  273.14  281.92  288.94  295.93  295.45  299.36  307.17  \n",
       "17    195.28  200.23  205.84  206.98  216.40  222.95  220.43  229.60  231.14  \n",
       "18    194.98  198.42  201.12  207.39  212.63  219.06  219.74  222.42  229.08  \n",
       "19    189.92  192.69  192.40  195.51  201.62  200.75  200.82  204.29  207.10  \n",
       "20    187.48  189.09  187.96  194.14  197.54  199.45  208.08  213.49  214.69  \n",
       "21    167.91  172.87  171.30  178.46  181.97  181.57  183.93  179.64  186.97  \n",
       "22    306.19  309.86  312.22  316.78  324.83  332.01  331.96  339.75  345.77  \n",
       "23    201.98  203.10  206.67  208.27  213.09  216.09  217.78  220.23  221.73  \n",
       "24    222.90  225.40  228.31  230.44  232.02  232.30  231.61  234.64  238.80  \n",
       "25    180.21  178.96  188.48  195.19  190.16  191.36  196.28  206.68  195.59  \n",
       "26    290.38  295.61  299.90  303.60  311.80  316.09  309.72  314.74  323.23  \n",
       "27    215.68  219.68  220.65  221.35  224.00  225.53  225.02  228.13  230.37  \n",
       "28    163.21  169.67  163.91  164.88  170.94  175.89  177.54  176.19  183.08  \n",
       "29    154.01  151.38  149.58  155.49  154.99  161.88  162.88  163.75  165.98  \n",
       "..       ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "374   257.81  260.78  266.58  271.23  282.93  286.98  289.09  289.08  292.91  \n",
       "375   218.43  217.97  212.68  222.43  224.83  222.60  218.64  230.12  226.77  \n",
       "376   170.61  164.75  174.03  170.10  168.10  170.20  174.18  176.70  186.43  \n",
       "377   223.83  226.55  227.42  227.73  230.37  232.75  229.62  236.06  237.41  \n",
       "378   191.84  195.87  200.58  201.28  208.14  209.39  208.96  212.37  216.06  \n",
       "379   212.18  216.39  218.66  227.03  233.59  236.57  236.28  236.91  244.00  \n",
       "380   229.61  230.66  234.90  233.06  235.22  244.91  254.88  258.83  261.58  \n",
       "381   143.97  150.61  146.80  152.81  154.64  157.75  155.19  152.04  163.40  \n",
       "382   171.78  175.06  176.84  179.27  184.99  188.72  188.88  190.39  194.76  \n",
       "383   266.42  267.44  269.77  273.02  277.13  280.63  282.95  284.43  290.45  \n",
       "384   219.97  223.61  222.63  228.05  226.04  232.83  234.72  230.79  234.19  \n",
       "385   202.65  189.39  212.59  222.38  198.94  214.71  220.51  226.82  226.32  \n",
       "386   172.59  174.58  176.59  178.78  185.22  185.17  186.86  188.81  193.75  \n",
       "387   186.27  178.92  178.65  177.39  181.11  190.05  186.00  193.52  193.97  \n",
       "388   222.73  229.23  238.45  239.43  249.94  259.57  261.10  269.22  268.57  \n",
       "389   278.39  283.16  287.13  292.97  298.18  306.35  310.50  310.69  311.71  \n",
       "390   187.25  190.46  186.56  194.97  195.54  196.99  200.86  197.27  198.77  \n",
       "391   171.29  172.55  180.74  176.82  184.79  180.38  184.56  179.49  190.87  \n",
       "392   174.56  177.90  177.70  177.82  180.90  184.41  183.31  186.34  191.97  \n",
       "393   201.26  198.51  199.20  200.42  198.21  205.51  200.56  196.85  201.25  \n",
       "394   188.05  189.35  191.10  194.94  194.68  197.75  196.67  200.48  204.09  \n",
       "395   211.12  217.98  219.39  220.30  228.47  232.18  234.34  237.78  244.70  \n",
       "396   206.94  210.96  210.32  216.79  210.40  211.64  222.29  223.40  224.50  \n",
       "397   159.31  160.60  161.61  167.04  167.62  172.46  175.91  175.37  179.80  \n",
       "398   219.92  223.56  225.38  228.88  234.84  238.97  239.11  241.20  244.34  \n",
       "399   183.86  190.90  193.78  199.89  205.44  213.52  215.54  217.71  224.21  \n",
       "400   168.65  171.21  172.17  173.90  172.55  177.03  178.71  178.03  181.77  \n",
       "401   153.73  154.03  157.80  154.69  159.50  159.90  163.65  162.30  165.72  \n",
       "402   221.57  230.93  230.90  234.95  241.88  247.62  248.14  252.09  257.18  \n",
       "403   174.75  177.59  180.66  180.92  180.64  186.50  190.10  190.08  194.07  \n",
       "\n",
       "[404 rows x 179 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReturns = pd.DataFrame(columns=df.columns[2:])\n",
    "for index, row in df.iterrows():\n",
    "    nr = []\n",
    "    vals = row.values\n",
    "    for i in range(1,len(vals) - 1):\n",
    "        nr.append(math.log(vals[i + 1] / vals[i] / indice[df.columns[i]]))\n",
    "    logReturns = logReturns.append(pd.DataFrame([nr], columns=logReturns.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_prediction = 1 * 4\n",
    "data_usage = 8 * 4\n",
    "total_data = data_usage + future_prediction\n",
    "remove_quarters = future_prediction\n",
    "\n",
    "vtset = logReturns.columns[0:len(logReturns.columns) - remove_quarters]\n",
    "training_num = int(len(vtset) * .7)\n",
    "training_set = logReturns[vtset[0:training_num]]\n",
    "validation_set = logReturns[vtset[training_num:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(dataset, batch_size):\n",
    "    while True:\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        while len(x_batch) < batch_size:\n",
    "            idx = np.random.randint(len(dataset.columns) - total_data)\n",
    "            series = dataset.sample(n=1)[dataset.columns[idx:idx + total_data]].values[0]\n",
    "            if np.isnan(series[data_usage: ]).any(): continue\n",
    "            \n",
    "            changes = []\n",
    "            for ret in series[0:data_usage]:\n",
    "                changes.append([0, 1] if np.isnan(ret) else [ret, 0])\n",
    "            x_batch.append(np.array(changes).reshape(data_usage,2))\n",
    "            #y_batch.append((series[data_usage: ] * return_diff + return_min).prod())\n",
    "            y_batch.append(np.sum(series[data_usage: ]))\n",
    "        \n",
    "        yield (np.array(x_batch), np.array(y_batch))\n",
    "        \n",
    "batch_size = 128\n",
    "training_generator = batch_generator(training_set, batch_size)\n",
    "validation_generator = batch_generator(validation_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 50)                10600     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,651\n",
      "Trainable params: 10,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2, input_shape=(data_usage,2)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad18cce70f94fddb5660adae7fc98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0238WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.390641). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0259WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.220912). Check your callbacks.\n",
      "10/10 [==============================] - 6s 607ms/step - loss: 0.0260 - val_loss: 0.0204\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 5s 549ms/step - loss: 0.0268 - val_loss: 0.0205\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 0.0289 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111088). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111088). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104993). Check your callbacks.\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.0268 - val_loss: 0.0188\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0293WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.326792). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0276WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.198917). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 3s - loss: 0.0275WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.188120). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.185586). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.183053). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0277WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.177505). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0279WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.171957). Check your callbacks.\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 0.0280 - val_loss: 0.0205\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 6s - loss: 0.0297WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.195916). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0283WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.189650). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0285WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126489). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0274WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126489). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0275WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.189650). Check your callbacks.\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.0274 - val_loss: 0.0193\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 5s 550ms/step - loss: 0.0278 - val_loss: 0.0207\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0247WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.251817). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.124170). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.162919). Check your callbacks.\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.0263 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 553ms/step - loss: 0.0279 - val_loss: 0.0194\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 7s - loss: 0.0242WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.225007). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 6s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.253979). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0263WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.199519). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.168400). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.199519). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.168400). Check your callbacks.\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 0.0264 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 6s - loss: 0.0334WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.407875). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0308WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.304763). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0301WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.201651). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 3s - loss: 0.0288WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.201651). Check your callbacks.\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 0.0280 - val_loss: 0.0181\n",
      "Epoch 1/1\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0251WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.133060). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118583). Check your callbacks.\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.0260 - val_loss: 0.0201\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 614ms/step - loss: 0.0272 - val_loss: 0.0187\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 0.0272 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.247829). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0282WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111780). Check your callbacks.\n",
      "10/10 [==============================] - 5s 549ms/step - loss: 0.0271 - val_loss: 0.0208\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.341641). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.173359). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 3s - loss: 0.0279WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.190746). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.119428). Check your callbacks.\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.0276 - val_loss: 0.0211\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 593ms/step - loss: 0.0266 - val_loss: 0.0192\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.257249). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.170762). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.167505). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105112). Check your callbacks.\n",
      "10/10 [==============================] - 6s 551ms/step - loss: 0.0260 - val_loss: 0.0206\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.0275 - val_loss: 0.0184\n",
      "Epoch 1/1\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0277WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.139049). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0282WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.131693). Check your callbacks.\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 0.0279 - val_loss: 0.0213\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.208383). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0277WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.160106). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.176177). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0276WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.193045). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0271WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.209913). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.193045). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.160106). Check your callbacks.\n",
      "10/10 [==============================] - 6s 578ms/step - loss: 0.0267 - val_loss: 0.0184\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0263WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105686). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0259WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100064). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105686). Check your callbacks.\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.0256 - val_loss: 0.0208\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 6s - loss: 0.0276WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.293549). Check your callbacks.\n",
      "10/10 [==============================] - 6s 629ms/step - loss: 0.0270 - val_loss: 0.0189\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 579ms/step - loss: 0.0268 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111202). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.125661). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.132171). Check your callbacks.\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 0.0266 - val_loss: 0.0197\n",
      "Epoch 1/1\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107059). Check your callbacks.\n",
      "10/10 [==============================] - 6s 586ms/step - loss: 0.0277 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0260WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136791). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136791). Check your callbacks.\n",
      "10/10 [==============================] - 6s 612ms/step - loss: 0.0257 - val_loss: 0.0188\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 625ms/step - loss: 0.0271 - val_loss: 0.0214\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0253WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.165792). Check your callbacks.\n",
      "10/10 [==============================] - 6s 612ms/step - loss: 0.0269 - val_loss: 0.0192\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0298WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111785). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116605). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116605). Check your callbacks.\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 0.0265 - val_loss: 0.0184\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 5s 545ms/step - loss: 0.0264 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0276WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126596). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.126596). Check your callbacks.\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.0266 - val_loss: 0.0189\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 6s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.291811). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.214662). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0271WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.195766). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0274WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121576). Check your callbacks.\n",
      "10/10 [==============================] - 5s 518ms/step - loss: 0.0270 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 0.0268 - val_loss: 0.0194\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 614ms/step - loss: 0.0249 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 3s - loss: 0.0271WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143899). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 2s - loss: 0.0263WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109182). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143899). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.109182). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143899). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.161888). Check your callbacks.\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 0.0267 - val_loss: 0.0194\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0283WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.271918). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.203561). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.271918). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.282000). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.271918). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.203561). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0251WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.271918). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0256WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.267538). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.263158). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.206657). Check your callbacks.\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 0.0259 - val_loss: 0.0201\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.0277 - val_loss: 0.0197\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 9s - loss: 0.0248WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.188208). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0244WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.103492). Check your callbacks.\n",
      "10/10 [==============================] - 6s 581ms/step - loss: 0.0258 - val_loss: 0.0199\n",
      "Epoch 1/1\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.118842). Check your callbacks.\n",
      "10/10 [==============================] - 7s 654ms/step - loss: 0.0260 - val_loss: 0.0204\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 6s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.119637). Check your callbacks.\n",
      "10/10 [==============================] - 6s 613ms/step - loss: 0.0265 - val_loss: 0.0197\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 550ms/step - loss: 0.0266 - val_loss: 0.0188\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.260523). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.150078). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0259WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.114321). Check your callbacks.\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 0.0262 - val_loss: 0.0207\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0287WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.286876). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.289239). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0263WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.286876). Check your callbacks.\n",
      "10/10 [==============================] - 6s 557ms/step - loss: 0.0263 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.0251 - val_loss: 0.0199\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 8s - loss: 0.0234WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.230937). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 6s - loss: 0.0242WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.156800). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.103197). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.116109). Check your callbacks.\n",
      "10/10 [==============================] - 6s 592ms/step - loss: 0.0273 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.140687). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138714). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136742). Check your callbacks.\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 0.0281 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.204483). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.164201). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.189995). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.187809). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.168936). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.150062). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143142). Check your callbacks.\n",
      "10/10 [==============================] - 6s 554ms/step - loss: 0.0264 - val_loss: 0.0189\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 0.0267 - val_loss: 0.0201\n",
      "Epoch 1/1\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0279WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.133464). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0277WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.127565). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0274WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.122912). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.121666). Check your callbacks.\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 0.0269 - val_loss: 0.0188\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 0.0257 - val_loss: 0.0185\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 5s 525ms/step - loss: 0.0268 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 0.0270 - val_loss: 0.0185\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.134429). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.179365). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0254WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.134429). Check your callbacks.\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.0259 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0294WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.232852). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0258WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.195522). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.158192). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.195522). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.158192). Check your callbacks.\n",
      "10/10 [==============================] - 6s 586ms/step - loss: 0.0273 - val_loss: 0.0201\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0275WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.183522). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.142460). Check your callbacks.\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 0.0269 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 3s - loss: 0.0290WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.280940). Check your callbacks.\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.0264 - val_loss: 0.0186\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0288WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.141841). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0282WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.149213). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.176202). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.162707). Check your callbacks.\n",
      "10/10 [==============================] - 5s 546ms/step - loss: 0.0266 - val_loss: 0.0201\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0230WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.187851). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0243WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.206581). Check your callbacks.\n",
      "10/10 [==============================] - 6s 614ms/step - loss: 0.0262 - val_loss: 0.0186\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0233WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.149034). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.180301). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0274WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.187218). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0285WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.180301). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0281WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.173384). Check your callbacks.\n",
      "10/10 [==============================] - 5s 533ms/step - loss: 0.0269 - val_loss: 0.0211\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 2s - loss: 0.0223WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.196480). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 3s - loss: 0.0243WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.192789). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0251WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.196480). Check your callbacks.\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.0262 - val_loss: 0.0187\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0249WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.191145). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0256WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.233165). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0259WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.191145). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.134315). Check your callbacks.\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 0.0271 - val_loss: 0.0200\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.205161). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0256WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.210068). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 3s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.220131). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.210068). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.200004). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.195097). Check your callbacks.\n",
      "10/10 [==============================] - 6s 638ms/step - loss: 0.0268 - val_loss: 0.0200\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0282WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.137583). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 3s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.164201). Check your callbacks.\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 0.0262 - val_loss: 0.0190\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 5s - loss: 0.0253WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.147073). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.187333). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 1s - loss: 0.0257WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.104490). Check your callbacks.\n",
      "10/10 [==============================] - 6s 625ms/step - loss: 0.0269 - val_loss: 0.0195\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.0257 - val_loss: 0.0190\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 0.0282 - val_loss: 0.0193\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.209796). Check your callbacks.\n",
      "10/10 [==============================] - 6s 551ms/step - loss: 0.0259 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 8s - loss: 0.0239WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.340930). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 6s - loss: 0.0246WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.212682). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0252WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100519). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.100519). Check your callbacks.\n",
      "10/10 [==============================] - 6s 564ms/step - loss: 0.0257 - val_loss: 0.0193\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.0256 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 3s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.305017). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0284WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.221421). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.173369). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0279WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138907). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0277WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138907). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.108079). Check your callbacks.\n",
      "10/10 [==============================] - 5s 549ms/step - loss: 0.0272 - val_loss: 0.0187\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0316WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.172961). Check your callbacks.\n",
      "10/10 [==============================] - 6s 615ms/step - loss: 0.0280 - val_loss: 0.0203\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 611ms/step - loss: 0.0261 - val_loss: 0.0205\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 0.0283 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 645ms/step - loss: 0.0263 - val_loss: 0.0197\n",
      "Epoch 1/1\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.0274 - val_loss: 0.0199\n",
      "Epoch 1/1\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0260WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.101350). Check your callbacks.\n",
      "10/10 [==============================] - 6s 626ms/step - loss: 0.0261 - val_loss: 0.0196\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0255WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.222463). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.225623). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0266WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.203370). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0268WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.184278). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.169727). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0271WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.184278). Check your callbacks.\n",
      "10/10 [==============================] - 6s 570ms/step - loss: 0.0271 - val_loss: 0.0192\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 3s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.129720). Check your callbacks.\n",
      "10/10 [==============================] - 6s 568ms/step - loss: 0.0264 - val_loss: 0.0206\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.246325). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0253WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.321667). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.132760). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0260WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.123885). Check your callbacks.\n",
      "10/10 [==============================] - 6s 622ms/step - loss: 0.0258 - val_loss: 0.0205\n",
      "Epoch 1/1\n",
      " 2/10 [=====>........................] - ETA: 3s - loss: 0.0260WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.122749). Check your callbacks.\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0262WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.105368). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0256WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138042). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0261WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.143917). Check your callbacks.\n",
      "10/10 [==============================] - 5s 532ms/step - loss: 0.0259 - val_loss: 0.0193\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.201007). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0270WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.201007). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.136732). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0267WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.174316). Check your callbacks.\n",
      "10/10 [==============================] - 6s 609ms/step - loss: 0.0268 - val_loss: 0.0194\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 4s - loss: 0.0287WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.185449). Check your callbacks.\n",
      " 2/10 [=====>........................] - ETA: 4s - loss: 0.0296WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.178048). Check your callbacks.\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0290WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.170648). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0284WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.141092). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111536). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0276WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.111536). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0273WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.138170). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0274WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.164803). Check your callbacks.\n",
      "10/10 [==============================] - 6s 582ms/step - loss: 0.0273 - val_loss: 0.0197\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 4s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107952). Check your callbacks.\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 0.0269 - val_loss: 0.0198\n",
      "Epoch 1/1\n",
      " 4/10 [===========>..................] - ETA: 3s - loss: 0.0263WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.146449). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0264WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.213540). Check your callbacks.\n",
      " 6/10 [=================>............] - ETA: 2s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.245186). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0269WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.276833). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0272WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.273862). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0271WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.270892). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.242216). Check your callbacks.\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.0272 - val_loss: 0.0191\n",
      "Epoch 1/1\n",
      " 1/10 [==>...........................] - ETA: 5s - loss: 0.0234WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.115046). Check your callbacks.\n",
      " 5/10 [==============>...............] - ETA: 2s - loss: 0.0280WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.180875). Check your callbacks.\n",
      " 7/10 [====================>.........] - ETA: 1s - loss: 0.0275WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.176014). Check your callbacks.\n",
      " 8/10 [=======================>......] - ETA: 1s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.178444). Check your callbacks.\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0278WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.176014). Check your callbacks.\n",
      "WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.156391). Check your callbacks.\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 0.0275 - val_loss: 0.0195\n",
      "Epoch 1/1\n",
      " 3/10 [========>.....................] - ETA: 3s - loss: 0.0265WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.106138). Check your callbacks.\n",
      "10/10 [==============================] - 6s 644ms/step - loss: 0.0289 - val_loss: 0.0195\n",
      "Epoch 1/1\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0262"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-eddad52cc827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         validation_data=validation_generator)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    219\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             val_outs = evaluate_generator(\n\u001b[0;32m--> 221\u001b[0;31m                 model, validation_generator, validation_steps, workers=0)\n\u001b[0m\u001b[1;32m    222\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    315\u001b[0m                          \u001b[0;34m'(x, y, sample_weight) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                          'or (x, y). Found: ' + str(generator_output))\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2807\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     updated = session.run(\n\u001b[0;32m-> 2809\u001b[0;31m         fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2810\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in tqdm_notebook(range(100)):\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        epochs=1,\n",
    "                        steps_per_epoch=10,\n",
    "                        validation_steps=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(logReturns.columns)\n",
    "x_raw = logReturns[logReturns.columns[n - total_data: n - future_prediction]].as_matrix()\n",
    "x_test = []\n",
    "for example in x_raw: \n",
    "    nans = np.isnan(example)\n",
    "    example[nans] = 0\n",
    "    x_test.append(np.array(np.column_stack((example,nans))))\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = math.e ** model.predict(x_test)[:,0]\n",
    "real = math.e ** logReturns[logReturns.columns[-future_prediction:]].as_matrix().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = np.array(list(zip(real,predict)))\n",
    "zipped = zipped[~np.isnan(zipped).any(axis=1)]\n",
    "predict = zipped[:,1]\n",
    "real = zipped[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.16631433989156436\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "bslope, bintercept, br_value, bp_value, bstd_err = stats.linregress(predict,real)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(predict,real)\n",
    "print('r2',r_value ** 2)\n",
    "\n",
    "line = slope*predict+intercept\n",
    "plt.plot(predict,real,'o', predict, line)\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('real')\n",
    "#plt.scatter(predict, real)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
